# -*- coding: utf-8 -*-
"""CL at rs=150 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11QD6Q9kHodnvgrmiz_5wwyhJq8DKEzBL
"""

#ridge
!pip install scikit-optimize
import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer
from scipy.stats import pearsonr

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx'
df = pd.read_excel(url)
#Rename the columns for better readability
df.columns = ['Relative_Compactness', 'Surface_Area', 'Wall_Area', 'Roof_Area', 'Overall_Height',
'Orientation', 'Glazing_Area', 'Glazing_Area_Distribution', 'Heating_Load', 'Cooling_Load']

#Split the data into training and testing sets
train_data, test_data, train_target, test_target = train_test_split(df.iloc[:, :-2], df.iloc[:, -2:],
test_size=0.3, random_state=150)


# Define the Ridge model
ridge_model = Ridge()

# Define the hyperparameter search space
search_space = {
    'alpha': Real(0.01, 100, prior='log-uniform'),
    'fit_intercept': [True, False],
    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
}

# Define the Bayesian optimization search
bayes_search = BayesSearchCV(estimator=ridge_model, search_spaces=search_space, n_iter=25,
                             scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1, random_state=150)

# Fit the Bayesian optimization search to the training data
bayes_search.fit(train_data, train_target.iloc[:, -1])

# Print the best hyperparameters found by the Bayesian optimization search
print('Best Hyperparameters:', bayes_search.best_params_)

# Evaluate the Ridge model on the testing data using the best hyperparameters found
best_ridge_model = bayes_search.best_estimator_
preds = best_ridge_model.predict(test_data)
rmse = mean_squared_error(test_target.iloc[:, 0], preds, squared=False)
mae = mean_absolute_error(test_target.iloc[:, 0], preds)
r2 = r2_score(test_target.iloc[:, 0], preds)
pi = pearsonr(test_target.iloc[:, 0], preds)[0] ** 2

print("RMSE:", rmse)
print("MAE:", mae)
print("R2 Score:", r2)
print("Pearson's R^2:", pi)

# Save the results in Google Colab
result_df_knn = pd.DataFrame({'y_test': test_target, 'y_predict': preds})
result_df_knn.to_excel('/content/Ridge_predictions.xlsx', index=False)

# Provide a download link
files.download('/content/Ridge_predictions.xlsx')

from sklearn.svm import SVR

# Split data into training and test sets
train_data, test_data, train_target, test_target = train_test_split(df.iloc[:, :-1], df.iloc[:, -1],
                                                                    test_size=0.3, random_state=150)

# Define the SVR model
svr_model = SVR()

# Define the search space for hyperparameters
search_space = {
    'C': Real(1e-2, 1e+2, prior='log-uniform'),
    'gamma': Real(0.001, 1, prior='log-uniform'),
    'epsilon': Real(0.01, 1)
}

# Define the Bayesian optimization search
bayes_cv_tuner = BayesSearchCV(
    estimator=svr_model,
    search_spaces=search_space,
    scoring='neg_mean_squared_error',
    n_iter=10,  # Adjusted number of iterations
    cv=5,
    verbose=1,
    random_state=150,
    n_jobs=-1
)

# Fit the Bayesian optimization search
bayes_cv_tuner.fit(train_data, train_target)

# Print the best hyperparameters
print("Best hyperparameters found by Bayesian optimization search:")
print(bayes_cv_tuner.best_params_)

# Create a SVR model with the best hyperparameters
best_svr_model = SVR(**bayes_cv_tuner.best_params_)

# Train the SVR model
best_svr_model.fit(train_data, train_target)

# Predict on the test set
test_preds1 = best_svr_model.predict(test_data)

# Calculate metrics
test_rmse = mean_squared_error(test_target, test_preds1, squared=False)
test_mse = mean_squared_error(test_target, test_preds1)
test_mae = mean_absolute_error(test_target, test_preds1)
test_r2 = r2_score(test_target, test_preds1)

print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MSE: {test_mse:.2f}")
print(f"Test MAE: {test_mae:.2f}")
print(f"Test R^2: {test_r2:.2f}")

# Save the results in Google Colab
result_df_knn = pd.DataFrame({'y_test': test_target, 'y_predict': test_preds1})
result_df_knn.to_excel('/content/svr_predictions1.xlsx', index=False)

# Provide a download link
files.download('/content/svr_predictions1.xlsx')

from sklearn.neighbors import KNeighborsRegressor

# Split data into training and test sets
train_data, test_data, train_target, test_target = train_test_split(df.iloc[:, :-1], df.iloc[:, -1],
                                                                    test_size=0.3, random_state=150)

# Define the KNN model
knn_model = KNeighborsRegressor()

# Define the search space for hyperparameters
search_space = {
    'n_neighbors': Integer(1, 20),
    'weights': ['uniform', 'distance'],
    'p': Integer(1, 2)
}

# Define the Bayesian optimization search
bayes_cv_tuner = BayesSearchCV(
    estimator=knn_model,
    search_spaces=search_space,
    scoring='neg_mean_squared_error',
    n_iter=10,  # Adjusted number of iterations
    cv=5,
    verbose=1,
    random_state=150,
    n_jobs=-1
)

# Fit the Bayesian optimization search
bayes_cv_tuner.fit(train_data, train_target)

# Print the best hyperparameters
print("Best hyperparameters found by Bayesian optimization search:")
print(bayes_cv_tuner.best_params_)

# Create a KNN model with the best hyperparameters
best_knn_model = KNeighborsRegressor(**bayes_cv_tuner.best_params_)

# Train the KNN model
best_knn_model.fit(train_data, train_target)

# Predict on the test set
test_preds = best_knn_model.predict(test_data)

# Calculate metrics
test_rmse = mean_squared_error(test_target, test_preds, squared=False)
test_mse = mean_squared_error(test_target, test_preds)
test_mae = mean_absolute_error(test_target, test_preds)
test_r2 = r2_score(test_target, test_preds)

print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MSE: {test_mse:.2f}")
print(f"Test MAE: {test_mae:.2f}")
print(f"Test R^2: {test_r2:.2f}")

from sklearn.neural_network import MLPRegressor
import numpy as np
from sklearn.preprocessing import StandardScaler

# Standardize the input features
scaler = StandardScaler()
train_data = scaler.fit_transform(train_data)
test_data = scaler.transform(test_data)

# Define the ANN model
ann_model = MLPRegressor(hidden_layer_sizes=(50, 50), activation='relu', solver='adam', random_state=150)

# Train the ANN model
ann_model.fit(train_data, train_target)

# Predict on the test set
test_preds2 = ann_model.predict(test_data)

# Calculate metrics
test_rmse = np.sqrt(mean_squared_error(test_target, test_preds2))
test_mse = mean_squared_error(test_target, test_preds2)
test_mae = mean_absolute_error(test_target, test_preds2)
test_r2 = r2_score(test_target, test_preds2)

print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MSE: {test_mse:.2f}")
print(f"Test MAE: {test_mae:.2f}")
print(f"Test R^2: {test_r2:.2f}")

# Save the results in Google Colab
result_df_knn = pd.DataFrame({'y_test': test_target, 'y_predict': test_preds2})
result_df_knn.to_excel('/content/ann_predictions1.xlsx', index=False)

# Provide a download link
files.download('/content/ann_predictions1.xlsx')

from sklearn.linear_model import Lasso
# Define the Lasso model
lasso_model = Lasso(alpha=1.0, random_state=150)

# Train the Lasso model
lasso_model.fit(train_data, train_target)

# Predict on the test set
test_preds3 = lasso_model.predict(test_data)

# Calculate metrics
test_rmse = mean_squared_error(test_target, test_preds3, squared=False)
test_mse = mean_squared_error(test_target, test_preds3)
test_mae = mean_absolute_error(test_target, test_preds3)
test_r2 = r2_score(test_target, test_preds3)

print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MSE: {test_mse:.2f}")
print(f"Test MAE: {test_mae:.2f}")
print(f"Test R^2: {test_r2:.2f}")

# Save the results in Google Colab
result_df_knn = pd.DataFrame({'y_test': test_target, 'y_predict': test_preds3})
result_df_knn.to_excel('/content/lasso_predictions1.xlsx', index=False)

# Provide a download link
files.download('/content/lasso_predictions1.xlsx')

from sklearn.tree import DecisionTreeRegressor
# Define the Decision Tree Regressor model
dt_model = DecisionTreeRegressor(random_state=150)

# Define the search space for hyperparameters
search_space = {
    'max_depth': Integer(1, 10),
    'min_samples_split': Integer(2, 20),
    'min_samples_leaf': Integer(1, 10),
    'max_features': Real(0.1, 1.0)
}

# Define the Bayesian optimization search
bayes_cv_tuner = BayesSearchCV(
    estimator=dt_model,
    search_spaces=search_space,
    scoring='neg_mean_squared_error',
    n_iter=10,  # Adjusted number of iterations
    cv=5,
    verbose=1,
    random_state=150,
    n_jobs=-1
)

# Fit the Bayesian optimization search
bayes_cv_tuner.fit(train_data, train_target)

# Print the best hyperparameters
print("Best hyperparameters found by Bayesian optimization search:")
print(bayes_cv_tuner.best_params_)

# Create a Decision Tree Regressor model with the best hyperparameters
best_dt_model = DecisionTreeRegressor(**bayes_cv_tuner.best_params_, random_state=150)

# Train the Decision Tree Regressor model
best_dt_model.fit(train_data, train_target)

# Predict on the test set
test_preds4 = best_dt_model.predict(test_data)

# Calculate metrics
test_rmse = mean_squared_error(test_target, test_preds4, squared=False)
test_mse = mean_squared_error(test_target, test_preds4)
test_mae = mean_absolute_error(test_target, test_preds4)
test_r2 = r2_score(test_target, test_preds4)

print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MSE: {test_mse:.2f}")
print(f"Test MAE: {test_mae:.2f}")
print(f"Test R^2: {test_r2:.2f}")

# Save the results in Google Colab
result_df_knn = pd.DataFrame({'y_test': test_target, 'y_predict': test_preds4})
result_df_knn.to_excel('/content/dt_predictions1.xlsx', index=False)

# Provide a download link
files.download('/content/dt_predictions1.xlsx')

from sklearn.ensemble import RandomForestRegressor
# Define the Random Forest Regressor model
rf_model = RandomForestRegressor(random_state=150)

# Define the search space for hyperparameters
search_space = {
    'n_estimators': Integer(50, 500),
    'max_depth': Integer(3, 10),
    'min_samples_split': Integer(2, 20),
    'min_samples_leaf': Integer(1, 10),
    'max_features': Real(0.1, 1.0)
}

# Define the Bayesian optimization search
bayes_cv_tuner = BayesSearchCV(
    estimator=rf_model,
    search_spaces=search_space,
    scoring='neg_mean_squared_error',
    n_iter=10,  # Adjusted number of iterations
    cv=5,
    verbose=1,
    random_state=150,
    n_jobs=-1
)

# Fit the Bayesian optimization search
bayes_cv_tuner.fit(train_data, train_target)

# Print the best hyperparameters
print("Best hyperparameters found by Bayesian optimization search:")
print(bayes_cv_tuner.best_params_)

# Create a Random Forest Regressor model with the best hyperparameters
best_rf_model = RandomForestRegressor(**bayes_cv_tuner.best_params_, random_state=150)

# Train the Random Forest Regressor model
best_rf_model.fit(train_data, train_target)

# Predict on the test set
test_preds5 = best_rf_model.predict(test_data)

# Calculate metrics
test_rmse = mean_squared_error(test_target, test_preds5, squared=False)
test_mse = mean_squared_error(test_target, test_preds5)
test_mae = mean_absolute_error(test_target, test_preds5)
test_r2 = r2_score(test_target, test_preds5)

print(f"Test RMSE: {test_rmse:.2f}")
print(f"Test MSE: {test_mse:.2f}")
print(f"Test MAE: {test_mae:.2f}")
print(f"Test R^2: {test_r2:.2f}")

# Assuming result_df is the DataFrame you want to save
result_df.to_excel('/content/random_forest_predictions.xlsx', index=False)

# Provide a download link
from google.colab import files
files.download('/content/random_forest_predictions.xlsx')

# Assuming result_df is the DataFrame you want to save
result_df.to_excel('/content/random_forest_predictions1.xlsx', index=False)

# Provide a download link
from google.colab import files
files.download('/content/random_forest_predictions1.xlsx')





